# Gu√≠a de Instalaci√≥n: AI SaaS Offline para Windows 11

**Optimizado para**: Windows 11, 40GB RAM, GPU 6GB (RTX 2060/2070 Super)

---

## üìã Tabla de Contenidos

1. [Requisitos Previos](#requisitos-previos)
2. [Instalaci√≥n de Componentes Base](#instalaci√≥n-de-componentes-base)
3. [Configuraci√≥n de Ollama](#configuraci√≥n-de-ollama)
4. [Instalaci√≥n de Modelos de IA](#instalaci√≥n-de-modelos-de-ia)
5. [Setup del Backend Python](#setup-del-backend-python)
6. [Configuraci√≥n de Stable Diffusion](#configuraci√≥n-de-stable-diffusion)
7. [Instalaci√≥n de Herramientas de Audio](#instalaci√≥n-de-herramientas-de-audio)
8. [Verificaci√≥n del Sistema](#verificaci√≥n-del-sistema)
9. [Troubleshooting](#troubleshooting)

---

## Requisitos Previos

Antes de comenzar, aseg√∫rate de tener instalado:

- **Windows 11** (versi√≥n 22H2 o superior)
- **Git** (descargable desde https://git-scm.com/)
- **Python 3.11+** (descargable desde https://www.python.org/)
- **Node.js 20+** (descargable desde https://nodejs.org/)
- **Visual C++ Redistributable** (necesario para algunos componentes)
- **CUDA Toolkit 12.1** (para GPU NVIDIA - descargable desde https://developer.nvidia.com/cuda-toolkit)

### Verificar Instalaciones

Abre PowerShell y ejecuta:

```powershell
python --version
node --version
git --version
```

---

## Instalaci√≥n de Componentes Base

### Paso 1: Crear Carpeta de Proyecto

```powershell
mkdir C:\AI-SaaS
cd C:\AI-SaaS
```

### Paso 2: Descargar e Instalar Ollama

1. Ve a https://ollama.com/download
2. Descarga el instalador para Windows
3. Ejecuta el instalador y sigue las instrucciones
4. Ollama se instalar√° en `C:\Users\[TuUsuario]\AppData\Local\Programs\Ollama`

**Verificar instalaci√≥n:**

```powershell
ollama --version
```

### Paso 3: Descargar e Instalar Git LFS (para modelos grandes)

```powershell
# Descargar desde https://git-lfs.com/
# O usar Chocolatey si est√° instalado:
choco install git-lfs
```

---

## Configuraci√≥n de Ollama

### Paso 1: Iniciar Servicio de Ollama

Ollama se ejecuta como servicio en Windows. Para verificar que est√° corriendo:

```powershell
# Verificar que el servicio est√° activo
Get-Service | Where-Object {$_.Name -like "*ollama*"}
```

Si no est√° corriendo, inicia Ollama manualmente desde el men√∫ de inicio.

### Paso 2: Configurar Variables de Entorno (Opcional)

Para optimizar el uso de GPU, crea un archivo `.env` en `C:\AI-SaaS`:

```
OLLAMA_NUM_GPU=1
OLLAMA_NUM_THREAD=8
OLLAMA_MAX_LOADED_MODELS=2
OLLAMA_KEEP_ALIVE=5m
```

---

## Instalaci√≥n de Modelos de IA

### Modelos Recomendados para tu GPU (6GB)

Abre PowerShell y ejecuta estos comandos uno por uno:

```powershell
# DeepSeek-R1 7B (4.7GB) - Excelente para razonamiento
ollama pull deepseek-r1:7b

# Qwen 2.5 Coder 7B (4.3GB) - Perfecto para c√≥digo
ollama pull qwen2.5-coder:7b

# Llama 2 7B (3.8GB) - Alternativa ligera
ollama pull llama2:7b

# Phi 4 Mini (2.5GB) - Muy r√°pido, bueno para chat
ollama pull phi4-mini
```

**Nota**: Estos comandos descargar√°n los modelos autom√°ticamente. Aseg√∫rate de tener al menos 100GB de espacio libre en tu disco.

### Verificar Modelos Instalados

```powershell
ollama list
```

---

## Setup del Backend Python

### Paso 1: Crear Entorno Virtual

```powershell
cd C:\AI-SaaS
python -m venv venv
.\venv\Scripts\Activate.ps1
```

### Paso 2: Instalar Dependencias

Crea un archivo `requirements.txt`:

```
fastapi==0.104.1
uvicorn==0.24.0
python-dotenv==1.0.0
requests==2.31.0
pillow==10.1.0
torch==2.1.1
torchvision==0.16.1
diffusers==0.24.0
transformers==4.35.2
accelerate==0.24.1
safetensors==0.4.1
pydantic==2.5.0
pydantic-settings==2.1.0
aiofiles==23.2.1
websockets==12.0
PyGithub==2.1.1
```

Instala las dependencias:

```powershell
pip install -r requirements.txt
```

**Nota**: La instalaci√≥n de `torch` con CUDA puede tardar varios minutos.

### Paso 3: Crear Estructura de Carpetas

```powershell
mkdir backend
mkdir backend\services
mkdir backend\models
mkdir backend\utils
mkdir frontend
mkdir data
mkdir data\projects
mkdir data\models
```

---

## Configuraci√≥n de Stable Diffusion

### Paso 1: Descargar Modelos

Dentro del entorno virtual de Python:

```powershell
python -c "
from diffusers import StableDiffusionXLPipeline
import torch

# Descargar modelo (primera vez toma tiempo)
pipe = StableDiffusionXLPipeline.from_pretrained(
    'stabilityai/stable-diffusion-xl-base-1.0',
    torch_dtype=torch.float16,
    use_safetensors=True
)
print('Modelo descargado exitosamente')
"
```

**Nota**: Este modelo ocupa ~7GB. Aseg√∫rate de tener espacio suficiente.

---

## Instalaci√≥n de Herramientas de Audio

### Paso 1: Instalar Whisper (Speech-to-Text)

```powershell
pip install openai-whisper
```

Descargar modelo:

```powershell
python -m whisper --model medium --language es --output_format txt "test.wav"
```

### Paso 2: Instalar Piper (Text-to-Speech)

Descarga desde: https://github.com/rhasspy/piper/releases

1. Descarga `piper_amd64.zip` para Windows
2. Extrae en `C:\AI-SaaS\tools\piper`
3. Descarga voces en espa√±ol:

```powershell
cd C:\AI-SaaS\tools\piper
# Descargar voz en espa√±ol
Invoke-WebRequest -Uri "https://huggingface.co/rhasspy/piper-voices/resolve/main/es/es_ES-sharvard-medium.onnx" -OutFile "models\es_ES-sharvard-medium.onnx"
```

---

## Verificaci√≥n del Sistema

Crea un script de prueba `test_setup.py`:

```python
import os
import subprocess
import requests
import torch
from pathlib import Path

print("üîç Verificando instalaci√≥n del sistema...\n")

# 1. Verificar Ollama
print("1Ô∏è‚É£  Ollama:")
try:
    response = requests.get("http://localhost:11434/api/tags")
    models = response.json().get("models", [])
    print(f"   ‚úÖ Ollama corriendo. Modelos: {len(models)}")
    for model in models[:3]:
        print(f"      - {model['name']}")
except:
    print("   ‚ùå Ollama no est√° corriendo. Inicia Ollama manualmente.")

# 2. Verificar Python
print("\n2Ô∏è‚É£  Python:")
print(f"   ‚úÖ Python {os.sys.version.split()[0]}")

# 3. Verificar PyTorch
print("\n3Ô∏è‚É£  PyTorch:")
print(f"   ‚úÖ PyTorch {torch.__version__}")
print(f"   ‚úÖ CUDA disponible: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"   ‚úÖ GPU: {torch.cuda.get_device_name(0)}")
    print(f"   ‚úÖ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")

# 4. Verificar Whisper
print("\n4Ô∏è‚É£  Whisper:")
try:
    result = subprocess.run(["whisper", "--version"], capture_output=True, text=True)
    print(f"   ‚úÖ Whisper instalado")
except:
    print("   ‚ùå Whisper no encontrado")

# 5. Verificar Piper
print("\n5Ô∏è‚É£  Piper:")
piper_path = Path("C:\\AI-SaaS\\tools\\piper\\piper.exe")
if piper_path.exists():
    print(f"   ‚úÖ Piper encontrado")
else:
    print(f"   ‚ùå Piper no encontrado en {piper_path}")

print("\n‚úÖ Verificaci√≥n completada")
```

Ejecuta:

```powershell
python test_setup.py
```

---

## Troubleshooting

### Problema: Ollama no inicia

**Soluci√≥n:**
1. Reinicia Windows
2. Abre PowerShell como administrador
3. Ejecuta: `net start Ollama`

### Problema: GPU no se detecta

**Soluci√≥n:**
1. Verifica que CUDA Toolkit 12.1 est√© instalado
2. Actualiza drivers NVIDIA: https://www.nvidia.com/Download/driverDetails.aspx
3. Ejecuta: `nvidia-smi` en PowerShell para verificar

### Problema: Falta memoria (OOM)

**Soluci√≥n:**
1. Reduce el tama√±o del modelo (usa versiones 1B o 3B)
2. Habilita `use_8bit=True` en Stable Diffusion
3. Reduce batch size en generaci√≥n de im√°genes

### Problema: Whisper muy lento

**Soluci√≥n:**
1. Usa modelo `small` en lugar de `medium`
2. Especifica idioma: `--language es`

---

## Pr√≥ximos Pasos

Una vez completada la instalaci√≥n:

1. Inicia Ollama
2. Ejecuta `test_setup.py` para verificar
3. Procede con la instalaci√≥n del Backend Python
4. Luego instala el Frontend React

**Tiempo estimado de instalaci√≥n**: 2-3 horas (dependiendo de velocidad de internet)

---

**Versi√≥n**: 1.0
**√öltima actualizaci√≥n**: 30 de diciembre de 2025
**Autor**: Manus AI
